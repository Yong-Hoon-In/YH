{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "newsclassfication123.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YNZSUqciltF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8acaa503-c04e-4a21-eed6-30768a66430f"
      },
      "source": [
        "!pip install transformers\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import transformers\n",
        "import sklearn\n",
        "import re\n",
        "from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 10.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=38f3a8ed589a07a83137b38c184a45c1531030c49ae01673aa42c45c9783d98b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqzg0etXit17"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou-oVlLnitxr"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(transformers.__version__)\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIjRpqD8z_D8"
      },
      "source": [
        "df_train = pd.read_csv('../Data/news_train.csv')\n",
        "df_test = pd.read_csv('../Data/news_test.csv')\n",
        "sample=pd.read_csv('../Data/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1uUXhUJitoC"
      },
      "source": [
        "df_train['merge']=df_train[['title','content']].apply(' '.join, axis=1)\n",
        "df_test['merge']=df_test[['title','content']].apply(' '.join, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d43x4f6litka"
      },
      "source": [
        "def preprocess(text):\n",
        "    text=re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',text)\n",
        "    text=re.sub(r'[^ .,?!/@$%~％·∼()\\x00-\\x7F가-힣]+','',text) #한자제거#\n",
        "    return text\n",
        "df_train['merge'] = df_train['merge'].apply(preprocess)\n",
        "df_test['merge'] = df_test['merge'].apply(preprocess)\n",
        "df_train=df_train[df_train[\"merge\"]!='']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3JmHZIWitgX"
      },
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLaa9n1jitUq"
      },
      "source": [
        "merged=df_train[['merge','info']]\n",
        "texts=merged['merge'].values\n",
        "labels=merged['info'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8lgqzuitRK"
      },
      "source": [
        "indices=tokenizer.batch_encode_plus(texts,max_length=100,add_special_tokens=True, return_attention_mask=True,padding='max_length',truncation=True)\n",
        "input_ids=indices[\"input_ids\"]\n",
        "attention_masks=indices[\"attention_mask\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sem0Co2kitM5"
      },
      "source": [
        "train_inputs = torch.tensor(input_ids)\n",
        "train_labels = torch.tensor(labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "batch_size = 32\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdp4d4CQi-kB"
      },
      "source": [
        "# Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n",
        "\n",
        "# https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "# and/or\n",
        "# https://github.com/lessw2020/Best-Deep-Learning-Optimizers\n",
        "\n",
        "# Ranger has been used to capture 12 records on the FastAI leaderboard.\n",
        "\n",
        "# This version = 2020.9.4\n",
        "\n",
        "\n",
        "# Credits:\n",
        "# Gradient Centralization --> https://arxiv.org/abs/2004.01461v2 (a new optimization technique for DNNs), github:  https://github.com/Yonghongwei/Gradient-Centralization\n",
        "# RAdam -->  https://github.com/LiyuanLucasLiu/RAdam\n",
        "# Lookahead --> rewritten by lessw2020, but big thanks to Github @LonePatient and @RWightman for ideas from their code.\n",
        "# Lookahead paper --> MZhang,G Hinton  https://arxiv.org/abs/1907.08610\n",
        "\n",
        "# summary of changes:\n",
        "# 9/4/20 - updated addcmul_ signature to avoid warning.  Integrates latest changes from GC developer (he did the work for this), and verified on performance on private dataset.\n",
        "# 4/11/20 - add gradient centralization option.  Set new testing benchmark for accuracy with it, toggle with use_gc flag at init.\n",
        "# full code integration with all updates at param level instead of group, moves slow weights into state dict (from generic weights),\n",
        "# supports group learning rates (thanks @SHolderbach), fixes sporadic load from saved model issues.\n",
        "# changes 8/31/19 - fix references to *self*.N_sma_threshold;\n",
        "# changed eps to 1e-5 as better default than 1e-8.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "\n",
        "def centralized_gradient(x, use_gc=True, gc_conv_only=False):\n",
        "    '''credit - https://github.com/Yonghongwei/Gradient-Centralization '''\n",
        "    if use_gc:\n",
        "        if gc_conv_only:\n",
        "            if len(list(x.size())) > 3:\n",
        "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
        "        else:\n",
        "            if len(list(x.size())) > 1:\n",
        "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
        "    return x\n",
        "\n",
        "\n",
        "class Ranger(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3,                       # lr\n",
        "                 alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n",
        "                 betas=(.9, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
        "                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
        "                 use_gc=True, gc_conv_only=False, gc_loc=True\n",
        "                 ):\n",
        "\n",
        "        # parameter checks\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        if not lr > 0:\n",
        "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
        "        if not eps > 0:\n",
        "            raise ValueError(f'Invalid eps: {eps}')\n",
        "\n",
        "        # parameter comments:\n",
        "        # beta1 (momentum) of .95 seems to work better than .90...\n",
        "        # N_sma_threshold of 5 seems better in testing than 4.\n",
        "        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
        "\n",
        "        # prep defaults and init torch.optim base\n",
        "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
        "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        # adjustable threshold\n",
        "        self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "        # look ahead params\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "\n",
        "        # radam buffer for state\n",
        "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
        "\n",
        "        # gc on or off\n",
        "        self.gc_loc = gc_loc\n",
        "        self.use_gc = use_gc\n",
        "        self.gc_conv_only = gc_conv_only\n",
        "        # level of gradient centralization\n",
        "        #self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
        "\n",
        "        print(\n",
        "            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n",
        "        if (self.use_gc and self.gc_conv_only == False):\n",
        "            print(f\"GC applied to both conv and fc layers\")\n",
        "        elif (self.use_gc and self.gc_conv_only == True):\n",
        "            print(f\"GC applied to conv layers only\")\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        print(\"set state called\")\n",
        "        super(Ranger, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n",
        "        # Uncomment if you need to use the actual closure...\n",
        "\n",
        "        # if closure is not None:\n",
        "        #loss = closure()\n",
        "\n",
        "        # Evaluate averages and grad, update param tensors\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        'Ranger optimizer does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]  # get state dict for this param\n",
        "\n",
        "                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
        "                    # if self.first_run_check==0:\n",
        "                    # self.first_run_check=1\n",
        "                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                    # look ahead weight storage now in state dict\n",
        "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
        "                    state['slow_buffer'].copy_(p.data)\n",
        "\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
        "                        p_data_fp32)\n",
        "\n",
        "                # begin computations\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # GC operation for Conv layers and FC layers\n",
        "                # if grad.dim() > self.gc_gradient_threshold:\n",
        "                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
        "                if self.gc_loc:\n",
        "                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # compute variance mov avg\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # compute mean moving avg\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * \\\n",
        "                        state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > self.N_sma_threshhold:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
        "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # if group['weight_decay'] != 0:\n",
        "                #    p_data_fp32.add_(-group['weight_decay']\n",
        "                #                     * group['lr'], p_data_fp32)\n",
        "\n",
        "                # apply lr\n",
        "                if N_sma > self.N_sma_threshhold:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    G_grad = exp_avg / denom\n",
        "                else:\n",
        "                    G_grad = exp_avg\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    G_grad.add_(p_data_fp32, alpha=group['weight_decay'])\n",
        "                # GC operation\n",
        "                if self.gc_loc == False:\n",
        "                    G_grad = centralized_gradient(G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n",
        "\n",
        "                p_data_fp32.add_(G_grad, alpha=-step_size * group['lr'])\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "                # integrated look ahead...\n",
        "                # we do it at the param level instead of group level\n",
        "                if state['step'] % group['k'] == 0:\n",
        "                    # get access to slow param tensor\n",
        "                    slow_p = state['slow_buffer']\n",
        "                    # (fast weights - slow weights) * alpha\n",
        "                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n",
        "                    # copy interpolated weights to RAdam param tensor\n",
        "                    p.data.copy_(slow_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEuz--VSi-f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0a4ff3-06ee-4f30-da60-f842e7913b26"
      },
      "source": [
        "optimizer = Ranger(model.parameters(),\n",
        "                  lr = 3e-4,#3e-4, 5e-5, 1e-4, 1.5e-4#1e-6,#6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-6#1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t_zqShSi-cZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXp1q-NbLyk"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbeEmU2i-Vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2e832b-4f1e-4677-dff0-4bc20e14f55a"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "      \n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    50  of  3,711.    Elapsed: 0:00:06.\n",
            "  Batch   100  of  3,711.    Elapsed: 0:00:12.\n",
            "  Batch   150  of  3,711.    Elapsed: 0:00:18.\n",
            "  Batch   200  of  3,711.    Elapsed: 0:00:24.\n",
            "  Batch   250  of  3,711.    Elapsed: 0:00:30.\n",
            "  Batch   300  of  3,711.    Elapsed: 0:00:36.\n",
            "  Batch   350  of  3,711.    Elapsed: 0:00:42.\n",
            "  Batch   400  of  3,711.    Elapsed: 0:00:48.\n",
            "  Batch   450  of  3,711.    Elapsed: 0:00:54.\n",
            "  Batch   500  of  3,711.    Elapsed: 0:01:01.\n",
            "  Batch   550  of  3,711.    Elapsed: 0:01:07.\n",
            "  Batch   600  of  3,711.    Elapsed: 0:01:13.\n",
            "  Batch   650  of  3,711.    Elapsed: 0:01:19.\n",
            "  Batch   700  of  3,711.    Elapsed: 0:01:25.\n",
            "  Batch   750  of  3,711.    Elapsed: 0:01:31.\n",
            "  Batch   800  of  3,711.    Elapsed: 0:01:38.\n",
            "  Batch   850  of  3,711.    Elapsed: 0:01:44.\n",
            "  Batch   900  of  3,711.    Elapsed: 0:01:50.\n",
            "  Batch   950  of  3,711.    Elapsed: 0:01:56.\n",
            "  Batch 1,000  of  3,711.    Elapsed: 0:02:02.\n",
            "  Batch 1,050  of  3,711.    Elapsed: 0:02:08.\n",
            "  Batch 1,100  of  3,711.    Elapsed: 0:02:15.\n",
            "  Batch 1,150  of  3,711.    Elapsed: 0:02:21.\n",
            "  Batch 1,200  of  3,711.    Elapsed: 0:02:27.\n",
            "  Batch 1,250  of  3,711.    Elapsed: 0:02:33.\n",
            "  Batch 1,300  of  3,711.    Elapsed: 0:02:40.\n",
            "  Batch 1,350  of  3,711.    Elapsed: 0:02:46.\n",
            "  Batch 1,400  of  3,711.    Elapsed: 0:02:52.\n",
            "  Batch 1,450  of  3,711.    Elapsed: 0:02:58.\n",
            "  Batch 1,500  of  3,711.    Elapsed: 0:03:04.\n",
            "  Batch 1,550  of  3,711.    Elapsed: 0:03:11.\n",
            "  Batch 1,600  of  3,711.    Elapsed: 0:03:17.\n",
            "  Batch 1,650  of  3,711.    Elapsed: 0:03:23.\n",
            "  Batch 1,700  of  3,711.    Elapsed: 0:03:29.\n",
            "  Batch 1,750  of  3,711.    Elapsed: 0:03:36.\n",
            "  Batch 1,800  of  3,711.    Elapsed: 0:03:42.\n",
            "  Batch 1,850  of  3,711.    Elapsed: 0:03:48.\n",
            "  Batch 1,900  of  3,711.    Elapsed: 0:03:55.\n",
            "  Batch 1,950  of  3,711.    Elapsed: 0:04:01.\n",
            "  Batch 2,000  of  3,711.    Elapsed: 0:04:07.\n",
            "  Batch 2,050  of  3,711.    Elapsed: 0:04:13.\n",
            "  Batch 2,100  of  3,711.    Elapsed: 0:04:20.\n",
            "  Batch 2,150  of  3,711.    Elapsed: 0:04:26.\n",
            "  Batch 2,200  of  3,711.    Elapsed: 0:04:32.\n",
            "  Batch 2,250  of  3,711.    Elapsed: 0:04:38.\n",
            "  Batch 2,300  of  3,711.    Elapsed: 0:04:45.\n",
            "  Batch 2,350  of  3,711.    Elapsed: 0:04:51.\n",
            "  Batch 2,400  of  3,711.    Elapsed: 0:04:57.\n",
            "  Batch 2,450  of  3,711.    Elapsed: 0:05:03.\n",
            "  Batch 2,500  of  3,711.    Elapsed: 0:05:10.\n",
            "  Batch 2,550  of  3,711.    Elapsed: 0:05:16.\n",
            "  Batch 2,600  of  3,711.    Elapsed: 0:05:22.\n",
            "  Batch 2,650  of  3,711.    Elapsed: 0:05:29.\n",
            "  Batch 2,700  of  3,711.    Elapsed: 0:05:35.\n",
            "  Batch 2,750  of  3,711.    Elapsed: 0:05:41.\n",
            "  Batch 2,800  of  3,711.    Elapsed: 0:05:47.\n",
            "  Batch 2,850  of  3,711.    Elapsed: 0:05:54.\n",
            "  Batch 2,900  of  3,711.    Elapsed: 0:06:00.\n",
            "  Batch 2,950  of  3,711.    Elapsed: 0:06:06.\n",
            "  Batch 3,000  of  3,711.    Elapsed: 0:06:12.\n",
            "  Batch 3,050  of  3,711.    Elapsed: 0:06:18.\n",
            "  Batch 3,100  of  3,711.    Elapsed: 0:06:25.\n",
            "  Batch 3,150  of  3,711.    Elapsed: 0:06:31.\n",
            "  Batch 3,200  of  3,711.    Elapsed: 0:06:37.\n",
            "  Batch 3,250  of  3,711.    Elapsed: 0:06:43.\n",
            "  Batch 3,300  of  3,711.    Elapsed: 0:06:49.\n",
            "  Batch 3,350  of  3,711.    Elapsed: 0:06:56.\n",
            "  Batch 3,400  of  3,711.    Elapsed: 0:07:02.\n",
            "  Batch 3,450  of  3,711.    Elapsed: 0:07:08.\n",
            "  Batch 3,500  of  3,711.    Elapsed: 0:07:14.\n",
            "  Batch 3,550  of  3,711.    Elapsed: 0:07:20.\n",
            "  Batch 3,600  of  3,711.    Elapsed: 0:07:26.\n",
            "  Batch 3,650  of  3,711.    Elapsed: 0:07:33.\n",
            "  Batch 3,700  of  3,711.    Elapsed: 0:07:39.\n",
            "\n",
            "  Average training loss: 0.07077\n",
            "  Training epoch took: 0:07:40\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    50  of  3,711.    Elapsed: 0:00:06.\n",
            "  Batch   100  of  3,711.    Elapsed: 0:00:12.\n",
            "  Batch   150  of  3,711.    Elapsed: 0:00:19.\n",
            "  Batch   200  of  3,711.    Elapsed: 0:00:25.\n",
            "  Batch   250  of  3,711.    Elapsed: 0:00:31.\n",
            "  Batch   300  of  3,711.    Elapsed: 0:00:37.\n",
            "  Batch   350  of  3,711.    Elapsed: 0:00:43.\n",
            "  Batch   400  of  3,711.    Elapsed: 0:00:49.\n",
            "  Batch   450  of  3,711.    Elapsed: 0:00:56.\n",
            "  Batch   500  of  3,711.    Elapsed: 0:01:02.\n",
            "  Batch   550  of  3,711.    Elapsed: 0:01:08.\n",
            "  Batch   600  of  3,711.    Elapsed: 0:01:14.\n",
            "  Batch   650  of  3,711.    Elapsed: 0:01:20.\n",
            "  Batch   700  of  3,711.    Elapsed: 0:01:27.\n",
            "  Batch   750  of  3,711.    Elapsed: 0:01:33.\n",
            "  Batch   800  of  3,711.    Elapsed: 0:01:39.\n",
            "  Batch   850  of  3,711.    Elapsed: 0:01:45.\n",
            "  Batch   900  of  3,711.    Elapsed: 0:01:51.\n",
            "  Batch   950  of  3,711.    Elapsed: 0:01:57.\n",
            "  Batch 1,000  of  3,711.    Elapsed: 0:02:03.\n",
            "  Batch 1,050  of  3,711.    Elapsed: 0:02:10.\n",
            "  Batch 1,100  of  3,711.    Elapsed: 0:02:16.\n",
            "  Batch 1,150  of  3,711.    Elapsed: 0:02:22.\n",
            "  Batch 1,200  of  3,711.    Elapsed: 0:02:28.\n",
            "  Batch 1,250  of  3,711.    Elapsed: 0:02:34.\n",
            "  Batch 1,300  of  3,711.    Elapsed: 0:02:40.\n",
            "  Batch 1,350  of  3,711.    Elapsed: 0:02:47.\n",
            "  Batch 1,400  of  3,711.    Elapsed: 0:02:53.\n",
            "  Batch 1,450  of  3,711.    Elapsed: 0:02:59.\n",
            "  Batch 1,500  of  3,711.    Elapsed: 0:03:05.\n",
            "  Batch 1,550  of  3,711.    Elapsed: 0:03:11.\n",
            "  Batch 1,600  of  3,711.    Elapsed: 0:03:17.\n",
            "  Batch 1,650  of  3,711.    Elapsed: 0:03:24.\n",
            "  Batch 1,700  of  3,711.    Elapsed: 0:03:30.\n",
            "  Batch 1,750  of  3,711.    Elapsed: 0:03:36.\n",
            "  Batch 1,800  of  3,711.    Elapsed: 0:03:42.\n",
            "  Batch 1,850  of  3,711.    Elapsed: 0:03:48.\n",
            "  Batch 1,900  of  3,711.    Elapsed: 0:03:54.\n",
            "  Batch 1,950  of  3,711.    Elapsed: 0:04:00.\n",
            "  Batch 2,000  of  3,711.    Elapsed: 0:04:07.\n",
            "  Batch 2,050  of  3,711.    Elapsed: 0:04:13.\n",
            "  Batch 2,100  of  3,711.    Elapsed: 0:04:19.\n",
            "  Batch 2,150  of  3,711.    Elapsed: 0:04:25.\n",
            "  Batch 2,200  of  3,711.    Elapsed: 0:04:31.\n",
            "  Batch 2,250  of  3,711.    Elapsed: 0:04:37.\n",
            "  Batch 2,300  of  3,711.    Elapsed: 0:04:43.\n",
            "  Batch 2,350  of  3,711.    Elapsed: 0:04:50.\n",
            "  Batch 2,400  of  3,711.    Elapsed: 0:04:56.\n",
            "  Batch 2,450  of  3,711.    Elapsed: 0:05:02.\n",
            "  Batch 2,500  of  3,711.    Elapsed: 0:05:08.\n",
            "  Batch 2,550  of  3,711.    Elapsed: 0:05:14.\n",
            "  Batch 2,600  of  3,711.    Elapsed: 0:05:20.\n",
            "  Batch 2,650  of  3,711.    Elapsed: 0:05:26.\n",
            "  Batch 2,700  of  3,711.    Elapsed: 0:05:33.\n",
            "  Batch 2,750  of  3,711.    Elapsed: 0:05:39.\n",
            "  Batch 2,800  of  3,711.    Elapsed: 0:05:45.\n",
            "  Batch 2,850  of  3,711.    Elapsed: 0:05:51.\n",
            "  Batch 2,900  of  3,711.    Elapsed: 0:05:57.\n",
            "  Batch 2,950  of  3,711.    Elapsed: 0:06:03.\n",
            "  Batch 3,000  of  3,711.    Elapsed: 0:06:10.\n",
            "  Batch 3,050  of  3,711.    Elapsed: 0:06:16.\n",
            "  Batch 3,100  of  3,711.    Elapsed: 0:06:22.\n",
            "  Batch 3,150  of  3,711.    Elapsed: 0:06:28.\n",
            "  Batch 3,200  of  3,711.    Elapsed: 0:06:34.\n",
            "  Batch 3,250  of  3,711.    Elapsed: 0:06:41.\n",
            "  Batch 3,300  of  3,711.    Elapsed: 0:06:47.\n",
            "  Batch 3,350  of  3,711.    Elapsed: 0:06:53.\n",
            "  Batch 3,400  of  3,711.    Elapsed: 0:06:59.\n",
            "  Batch 3,450  of  3,711.    Elapsed: 0:07:05.\n",
            "  Batch 3,500  of  3,711.    Elapsed: 0:07:11.\n",
            "  Batch 3,550  of  3,711.    Elapsed: 0:07:17.\n",
            "  Batch 3,600  of  3,711.    Elapsed: 0:07:24.\n",
            "  Batch 3,650  of  3,711.    Elapsed: 0:07:30.\n",
            "  Batch 3,700  of  3,711.    Elapsed: 0:07:36.\n",
            "\n",
            "  Average training loss: 0.01912\n",
            "  Training epoch took: 0:07:37\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    50  of  3,711.    Elapsed: 0:00:06.\n",
            "  Batch   100  of  3,711.    Elapsed: 0:00:12.\n",
            "  Batch   150  of  3,711.    Elapsed: 0:00:18.\n",
            "  Batch   200  of  3,711.    Elapsed: 0:00:25.\n",
            "  Batch   250  of  3,711.    Elapsed: 0:00:31.\n",
            "  Batch   300  of  3,711.    Elapsed: 0:00:37.\n",
            "  Batch   350  of  3,711.    Elapsed: 0:00:43.\n",
            "  Batch   400  of  3,711.    Elapsed: 0:00:49.\n",
            "  Batch   450  of  3,711.    Elapsed: 0:00:55.\n",
            "  Batch   500  of  3,711.    Elapsed: 0:01:02.\n",
            "  Batch   550  of  3,711.    Elapsed: 0:01:08.\n",
            "  Batch   600  of  3,711.    Elapsed: 0:01:14.\n",
            "  Batch   650  of  3,711.    Elapsed: 0:01:20.\n",
            "  Batch   700  of  3,711.    Elapsed: 0:01:26.\n",
            "  Batch   750  of  3,711.    Elapsed: 0:01:33.\n",
            "  Batch   800  of  3,711.    Elapsed: 0:01:39.\n",
            "  Batch   850  of  3,711.    Elapsed: 0:01:45.\n",
            "  Batch   900  of  3,711.    Elapsed: 0:01:51.\n",
            "  Batch   950  of  3,711.    Elapsed: 0:01:57.\n",
            "  Batch 1,000  of  3,711.    Elapsed: 0:02:03.\n",
            "  Batch 1,050  of  3,711.    Elapsed: 0:02:09.\n",
            "  Batch 1,100  of  3,711.    Elapsed: 0:02:16.\n",
            "  Batch 1,150  of  3,711.    Elapsed: 0:02:22.\n",
            "  Batch 1,200  of  3,711.    Elapsed: 0:02:28.\n",
            "  Batch 1,250  of  3,711.    Elapsed: 0:02:34.\n",
            "  Batch 1,300  of  3,711.    Elapsed: 0:02:40.\n",
            "  Batch 1,350  of  3,711.    Elapsed: 0:02:46.\n",
            "  Batch 1,400  of  3,711.    Elapsed: 0:02:52.\n",
            "  Batch 1,450  of  3,711.    Elapsed: 0:02:58.\n",
            "  Batch 1,500  of  3,711.    Elapsed: 0:03:05.\n",
            "  Batch 1,550  of  3,711.    Elapsed: 0:03:11.\n",
            "  Batch 1,600  of  3,711.    Elapsed: 0:03:17.\n",
            "  Batch 1,650  of  3,711.    Elapsed: 0:03:23.\n",
            "  Batch 1,700  of  3,711.    Elapsed: 0:03:29.\n",
            "  Batch 1,750  of  3,711.    Elapsed: 0:03:35.\n",
            "  Batch 1,800  of  3,711.    Elapsed: 0:03:41.\n",
            "  Batch 1,850  of  3,711.    Elapsed: 0:03:47.\n",
            "  Batch 1,900  of  3,711.    Elapsed: 0:03:54.\n",
            "  Batch 1,950  of  3,711.    Elapsed: 0:04:00.\n",
            "  Batch 2,000  of  3,711.    Elapsed: 0:04:06.\n",
            "  Batch 2,050  of  3,711.    Elapsed: 0:04:12.\n",
            "  Batch 2,100  of  3,711.    Elapsed: 0:04:18.\n",
            "  Batch 2,150  of  3,711.    Elapsed: 0:04:24.\n",
            "  Batch 2,200  of  3,711.    Elapsed: 0:04:30.\n",
            "  Batch 2,250  of  3,711.    Elapsed: 0:04:36.\n",
            "  Batch 2,300  of  3,711.    Elapsed: 0:04:43.\n",
            "  Batch 2,350  of  3,711.    Elapsed: 0:04:49.\n",
            "  Batch 2,400  of  3,711.    Elapsed: 0:04:55.\n",
            "  Batch 2,450  of  3,711.    Elapsed: 0:05:01.\n",
            "  Batch 2,500  of  3,711.    Elapsed: 0:05:07.\n",
            "  Batch 2,550  of  3,711.    Elapsed: 0:05:13.\n",
            "  Batch 2,600  of  3,711.    Elapsed: 0:05:19.\n",
            "  Batch 2,650  of  3,711.    Elapsed: 0:05:25.\n",
            "  Batch 2,700  of  3,711.    Elapsed: 0:05:31.\n",
            "  Batch 2,750  of  3,711.    Elapsed: 0:05:38.\n",
            "  Batch 2,800  of  3,711.    Elapsed: 0:05:44.\n",
            "  Batch 2,850  of  3,711.    Elapsed: 0:05:50.\n",
            "  Batch 2,900  of  3,711.    Elapsed: 0:05:56.\n",
            "  Batch 2,950  of  3,711.    Elapsed: 0:06:02.\n",
            "  Batch 3,000  of  3,711.    Elapsed: 0:06:08.\n",
            "  Batch 3,050  of  3,711.    Elapsed: 0:06:14.\n",
            "  Batch 3,100  of  3,711.    Elapsed: 0:06:20.\n",
            "  Batch 3,150  of  3,711.    Elapsed: 0:06:27.\n",
            "  Batch 3,200  of  3,711.    Elapsed: 0:06:33.\n",
            "  Batch 3,250  of  3,711.    Elapsed: 0:06:39.\n",
            "  Batch 3,300  of  3,711.    Elapsed: 0:06:45.\n",
            "  Batch 3,350  of  3,711.    Elapsed: 0:06:51.\n",
            "  Batch 3,400  of  3,711.    Elapsed: 0:06:57.\n",
            "  Batch 3,450  of  3,711.    Elapsed: 0:07:03.\n",
            "  Batch 3,500  of  3,711.    Elapsed: 0:07:09.\n",
            "  Batch 3,550  of  3,711.    Elapsed: 0:07:16.\n",
            "  Batch 3,600  of  3,711.    Elapsed: 0:07:22.\n",
            "  Batch 3,650  of  3,711.    Elapsed: 0:07:28.\n",
            "  Batch 3,700  of  3,711.    Elapsed: 0:07:34.\n",
            "\n",
            "  Average training loss: 0.00575\n",
            "  Training epoch took: 0:07:35\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7ZTo7P6AR15"
      },
      "source": [
        "PATH = \"model.pth\"\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9ugBLzIZuir"
      },
      "source": [
        "medel.load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT52EZDbi-OQ"
      },
      "source": [
        "test1 = df_test['merge'].values\n",
        "\n",
        "indices1=tokenizer.batch_encode_plus(test1,max_length=100,add_special_tokens=True, return_attention_mask=True,padding='max_length',truncation=True)\n",
        "input_ids1=indices1[\"input_ids\"]\n",
        "attention_masks1=indices1[\"attention_mask\"]\n",
        "\n",
        "prediction_inputs1= torch.tensor(input_ids1)\n",
        "prediction_masks1 = torch.tensor(attention_masks1)\n",
        "\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32 \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\n",
        "prediction_sampler1 = SequentialSampler(prediction_data1)\n",
        "prediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__060vGIi-LZ"
      },
      "source": [
        "total_loss = 0\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader1:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids1, b_input_mask1 = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs1 = model(b_input_ids1, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask1)\n",
        "\n",
        "  logits1 = outputs1[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits1 = logits1.detach().cpu().numpy()\n",
        "  \n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits1)\n",
        "\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRjgXTNjjXbR"
      },
      "source": [
        "sample=pd.read_csv('/content/drive/MyDrive/sample_submission.csv')\n",
        "submit=pd.DataFrame({'id':sample['id'].values.tolist(),'info':flat_predictions})\n",
        "submit.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_gsQRpNRjtX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}